{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "224fe8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import json\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "client = openai.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b703333b",
   "metadata": {},
   "source": [
    "Criação de função que o modelo vai poder usar para extrair informações. Na prática essa função poderia vir de outra API ou de um banco de dados.\n",
    "\n",
    "- JSON (JavaScript Object Notation) --> formato de texto leve (string) usado para representar dados estruturados (nesse caso, dicionários)\n",
    "\n",
    "- `json.dumps()` recebe um objeto Python (nesse caso, dicionário) e retorna ele como uma string. Assim, a função retorna a informação num formato padronizado e estruturado e facilita o entendimento de outros sistemas que esperam dados em JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a1ad702",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obter_temperatura_atual(local, unidade=\"celsius\"):\n",
    "    if \"são paulo\" in local.lower():\n",
    "        return json.dumps({\"local\": \"São Paulo\", \"temperatura\": \"32\", \"unidade\": unidade})\n",
    "    elif \"porto alegre\" in local.lower():\n",
    "        return json.dumps({\"local\": \"Porto Alegre\", \"temperatura\": \"25\", \"unidade\": unidade})\n",
    "    elif \"rio de janeiro\" in local.lower():\n",
    "        return json.dumps({\"local\": \"Rio de Janeiro\", \"temperatura\": \"35\", \"unidade\": unidade})\n",
    "    else:\n",
    "        return json.dumps({\"local\": local, \"temperatura\": \"unknown\", unidade: unidade})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5938e6",
   "metadata": {},
   "source": [
    "Para informar ao modelo que ferramentas podem ser usadas, a lista de dicionários `tools` é passada para ele como um parametro de `client.chat.completions.create()` e tem o papel de especificar as ferramentas disponíveis.\n",
    "\n",
    "- Cada função deve ser um elemento dessa lista, um dicionário. Cada dicionário da `lista tools` se refere a uma função e inclui:\n",
    "    - `type`: tipo da ferramenta. Nesse caso, function\n",
    "    - `function`: descreve tudo da função em questão. Também é um dicionário e contém:\n",
    "        - `name`: nome da função\n",
    "        - `description`: explicação da utilidade da função para que o modelo saiba para que usá-la\n",
    "        - `parameters`: descrição e informações sobre os argumentos da função. Também é um dicionário e inclui:\n",
    "            - `type`: tipo dos parâmetros. Frequentemente, objetos\n",
    "            - `required`: lista dos argumentos (strings) que precisam ser fornecidos, os que não possuem valor padrão na função\n",
    "            - `properties`: é outro dicionário em que cada chave se refere ao nome de um argumento. Cada chave está associada à outro dicionário que contém informações sobre o parâmetro em questão:\n",
    "                - `type`: tipo do dado do argumento (int, string, etc)\n",
    "                - `description`: descrição do significado daquele dado\n",
    "                - `enum`: se for o caso, enum se associa com uma lista de valores predefinidos, em caso de opções bem definidas de entradas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "31500fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [{\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"obter_temperatura_atual\",\n",
    "        \"description\": \"Obtém a temperatura atual em uma dada cidade\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"required\": [\"local\"],\n",
    "            \"properties\": {\n",
    "                \"local\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"O nome da cidade. Ex: São Paulo\"\n",
    "                },\n",
    "                \"unidade\": {\n",
    "                    \"type\": \"string\", \n",
    "                    \"enum\": [\"celsius\", \"fahrenheit\"]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b66316",
   "metadata": {},
   "source": [
    "- Ao chamar chat.completions.create a lista `tools` é passada como parâmetro para que o modelo possa acessá-las.\n",
    "- `tool_choice` define a escolha das ferramentas pelo modelo\n",
    "    - `auto`: deixa que a escolha seja feita automaticamente. é o valor padrão\n",
    "    - Para forçar o modelo a usar uma determinada função, tool_choice pode receber o nome da função."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "da5ad379",
   "metadata": {},
   "outputs": [],
   "source": [
    "mensagens = [\n",
    "    {\"role\": \"user\", \n",
    "     \"content\": \"Qual é a temperatura em São Paulo, Porto Alegre e Recife?\"}\n",
    "    ]\n",
    "\n",
    "resposta = client.chat.completions.create(\n",
    "    model       = \"gpt-4o-mini\",\n",
    "    messages    = mensagens,\n",
    "    tools       = tools,\n",
    "    tool_choice = \"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c865e856",
   "metadata": {},
   "source": [
    "- Verificando a resposta dada pelo modelo apenas com o código até aqui, percebe-se que o conteúdo está vazio (`content=None`), ou seja, o modelo não forneceu uma resposta em formato de texto\n",
    "- Por outro lado, o parâmetro `tool_calls` em `resposta.choices[0].message` agora possui um valor, o que mostra que o modelo está chamando funções externas para fornecer a resposta\n",
    "    - `tool_calls` mostra as chamadas de função que o modelo realizou para poder prosseguir com a resposta "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "36778fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_6YlcaTwb7JGGsG6iyJeHnDRU', function=Function(arguments='{\"local\": \"São Paulo\", \"unidade\": \"celsius\"}', name='obter_temperatura_atual'), type='function'), ChatCompletionMessageToolCall(id='call_NRwWmDhQ2oqZrPETVrno6LTM', function=Function(arguments='{\"local\": \"Porto Alegre\", \"unidade\": \"celsius\"}', name='obter_temperatura_atual'), type='function'), ChatCompletionMessageToolCall(id='call_o3Yj3tDKX2z6dXL20MI6sybH', function=Function(arguments='{\"local\": \"Recife\", \"unidade\": \"celsius\"}', name='obter_temperatura_atual'), type='function')], refusal=None)\n",
      "______________________________\n"
     ]
    }
   ],
   "source": [
    "print(resposta.choices[0].message)\n",
    "print('_'*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb312a89",
   "metadata": {},
   "source": [
    "Assim, as funções requisitadas devem ser chamadas e suas respostas atribuidas às mensagens com o role `tools` para que uma nova resposta seja gerada mas agora com essas informações já dispiníveis\n",
    "\n",
    "- Ao final, a segunda resposta é gerada com 5 mensagens em `menssagens`: A pergunta do usuário, a primeira resposta que o modelo gerou requisitando as ferramentas e a resposta dos três usos de ferramenta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7e4806af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'user', 'content': 'Qual é a temperatura em São Paulo, Porto Alegre e Recife?'}\n",
      "\n",
      "ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_6YlcaTwb7JGGsG6iyJeHnDRU', function=Function(arguments='{\"local\": \"São Paulo\", \"unidade\": \"celsius\"}', name='obter_temperatura_atual'), type='function'), ChatCompletionMessageToolCall(id='call_NRwWmDhQ2oqZrPETVrno6LTM', function=Function(arguments='{\"local\": \"Porto Alegre\", \"unidade\": \"celsius\"}', name='obter_temperatura_atual'), type='function'), ChatCompletionMessageToolCall(id='call_o3Yj3tDKX2z6dXL20MI6sybH', function=Function(arguments='{\"local\": \"Recife\", \"unidade\": \"celsius\"}', name='obter_temperatura_atual'), type='function')], refusal=None)\n",
      "\n",
      "{'role': 'tool', 'tool_call_id': 'call_6YlcaTwb7JGGsG6iyJeHnDRU', 'name': 'obter_temperatura_atual', 'content': '{\"local\": \"S\\\\u00e3o Paulo\", \"temperatura\": \"32\", \"unidade\": \"celsius\"}'}\n",
      "\n",
      "{'role': 'tool', 'tool_call_id': 'call_NRwWmDhQ2oqZrPETVrno6LTM', 'name': 'obter_temperatura_atual', 'content': '{\"local\": \"Porto Alegre\", \"temperatura\": \"25\", \"unidade\": \"celsius\"}'}\n",
      "\n",
      "{'role': 'tool', 'tool_call_id': 'call_o3Yj3tDKX2z6dXL20MI6sybH', 'name': 'obter_temperatura_atual', 'content': '{\"local\": \"Recife\", \"temperatura\": \"unknown\", \"celsius\": \"celsius\"}'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "funcoes_disponiveis = { \"obter_temperatura_atual\": obter_temperatura_atual }\n",
    "\n",
    "mensagem_resp = resposta.choices[0].message\n",
    "tool_calls = resposta.choices[0].message.tool_calls\n",
    "\n",
    "# se houve chamada de ferramentas\n",
    "if tool_calls:\n",
    "    mensagens.append(mensagem_resp)\n",
    "    \n",
    "    # para cada tool call\n",
    "    for tool_call in tool_calls:\n",
    "        # obtemos o nome e argumentos da função a ser chamada\n",
    "        # os argumentos são um json, .loads carrega como um dicionário\n",
    "        function_name = tool_call.function.name\n",
    "        function_args = json.loads(tool_call.function.arguments)\n",
    "        \n",
    "        # a variável function_to_call recebe a função a ser chamada,\n",
    "        # nome da função é a chave para o dicionário funcoes_disponiveis\n",
    "        function_to_call = funcoes_disponiveis[function_name]\n",
    "        \n",
    "        # chamamos a função com os argumentos\n",
    "        # .get para obter o valor do argumento, caso não exista, retorna None\n",
    "        function_response = function_to_call(\n",
    "            local   = function_args[\"local\"],\n",
    "            unidade = function_args[\"unidade\"],\n",
    "        )\n",
    "\n",
    "        # adicionamos a resposta da função no array de mensagens\n",
    "        # que será fornecido ao modelo para gerar a resposta final\n",
    "        mensagens.append(\n",
    "            {\n",
    "                # novo role: tool\n",
    "                \"role\": \"tool\",\n",
    "                \n",
    "                # deve ser adicionado para saber de qual chamada se trata,\n",
    "                # qual função foi chaamda e qual foi a resposta\n",
    "                \"tool_call_id\": tool_call.id,\n",
    "                \"name\": function_name,\n",
    "                \"content\": function_response,\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    for mensagem in mensagens:\n",
    "        print(mensagem, end='\\n\\n')\n",
    "        \n",
    "    # uma vez que todas as chamadas foram processadas, basta gerar a resposta final\n",
    "    segunda_resposta = client.chat.completions.create(\n",
    "        # só são necessários esses dois parâmetros\n",
    "        model    = \"gpt-4o-mini\",\n",
    "        messages = mensagens,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c3539f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atualmente, as temperaturas são as seguintes:\n",
      "\n",
      "- São Paulo: 32°C\n",
      "- Porto Alegre: 25°C\n",
      "- Recife: informação sobre a temperatura não disponível no momento. \n",
      "\n",
      "Se precisar de mais informações ou atualizações, fique à vontade para perguntar!\n"
     ]
    }
   ],
   "source": [
    "print(segunda_resposta.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
